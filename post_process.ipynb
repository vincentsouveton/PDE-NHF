{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8a11dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # hopefully runs on GPU\n",
    "folder_exp = 'models/model/' # path to trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a973934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET \n",
    "\n",
    "N_train = 20000\n",
    "N_val = 6384\n",
    "N_test = 6384\n",
    "d = 256 # number of particles in each simulation\n",
    "d_cond = 2 # number of information for describing a simulation\n",
    "\n",
    "Q000, P000 = np.load('data/Q00.npy'), np.load('data/P00.npy')\n",
    "Q010, P010 = np.load('data/Q05.npy'), np.load('data/P05.npy')\n",
    "Q020, P020 = np.load('data/Q10.npy'), np.load('data/P10.npy')\n",
    "Q030, P030 = np.load('data/Q15.npy'), np.load('data/P15.npy')\n",
    "Q040, P040 = np.load('data/Q20.npy'), np.load('data/P20.npy')\n",
    "Q050, P050 = np.load('data/Q25.npy'), np.load('data/P25.npy')\n",
    "Cond = np.load('data/Cond.npy')\n",
    "\n",
    "Q000_test_numpy, P000_test_numpy = Q000[N_train+N_val:N_train+N_val+N_test], P000[N_train+N_val:N_train+N_val+N_test]\n",
    "Q010_test_numpy, P010_test_numpy = Q010[N_train+N_val:N_train+N_val+N_test], P010[N_train+N_val:N_train+N_val+N_test]\n",
    "Q020_test_numpy, P020_test_numpy = Q020[N_train+N_val:N_train+N_val+N_test], P020[N_train+N_val:N_train+N_val+N_test]\n",
    "Q030_test_numpy, P030_test_numpy = Q030[N_train+N_val:N_train+N_val+N_test], P030[N_train+N_val:N_train+N_val+N_test]\n",
    "Q040_test_numpy, P040_test_numpy = Q040[N_train+N_val:N_train+N_val+N_test], P040[N_train+N_val:N_train+N_val+N_test]\n",
    "Q050_test_numpy, P050_test_numpy = Q050[N_train+N_val:N_train+N_val+N_test], P050[N_train+N_val:N_train+N_val+N_test]\n",
    "Cond_test_numpy = Cond[N_train+N_val:N_train+N_val+N_test]\n",
    "\n",
    "Q000_test, P000_test = torch.tensor(Q000_test_numpy,dtype=float), torch.tensor(P000_test_numpy,dtype=float)\n",
    "Q010_test, P010_test = torch.tensor(Q010_test_numpy,dtype=float), torch.tensor(P010_test_numpy,dtype=float)\n",
    "Q020_test, P020_test = torch.tensor(Q020_test_numpy,dtype=float), torch.tensor(P020_test_numpy,dtype=float)\n",
    "Q030_test, P030_test = torch.tensor(Q030_test_numpy,dtype=float), torch.tensor(P030_test_numpy,dtype=float)\n",
    "Q040_test, P040_test = torch.tensor(Q040_test_numpy,dtype=float), torch.tensor(P040_test_numpy,dtype=float)\n",
    "Q050_test, P050_test = torch.tensor(Q050_test_numpy,dtype=float), torch.tensor(P050_test_numpy,dtype=float)\n",
    "Cond_test = torch.tensor(Cond_test_numpy,dtype=float)\n",
    "\n",
    "Q000_test, P000_test = Q000_test.to(device), P000_test.to(device)\n",
    "Q010_test, P010_test = Q010_test.to(device), P010_test.to(device)\n",
    "Q020_test, P020_test = Q020_test.to(device), P020_test.to(device)\n",
    "Q030_test, P030_test = Q030_test.to(device), P030_test.to(device)\n",
    "Q040_test, P040_test = Q040_test.to(device), P040_test.to(device)\n",
    "Q050_test, P050_test = Q050_test.to(device), P050_test.to(device)\n",
    "Cond_test = Cond_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c253264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE MODEL\n",
    "\n",
    "class Potential(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.rho = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, q):  # q: (B, N)\n",
    "        q_centered = q - q.mean(dim=1, keepdim=True)  \n",
    "        phi_q = self.phi(q_centered.unsqueeze(-1))   \n",
    "        pooled = phi_q.sum(dim=1)                     \n",
    "        return self.rho(pooled).squeeze(-1)\n",
    "\n",
    "\n",
    "class NeuralHamiltonianFlow(nn.Module):\n",
    "    def __init__(self, L, dt):\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "        self.dt = dt\n",
    "        self.V_net = Potential()  # Scalar potential energy\n",
    "        self.register_parameter(name='a', param=torch.nn.Parameter(1.0*torch.ones(1))) # mass matrix scalar coefficient as M^-1 = a^2 Id\n",
    "\n",
    "    def potential_energy(self, q):\n",
    "        return self.V_net(q)\n",
    "\n",
    "    def leapfrog_integrator(self, q, p, L, dt):\n",
    "        # Compute initial grad_y (gradient of V)\n",
    "        V = self.potential_energy(q)\n",
    "        grad_q, = grad(V.sum(), q, create_graph=True)\n",
    "\n",
    "        for step in range(L):\n",
    "            # Half-step for momentum (kick)\n",
    "            p = p - 0.5 * dt * grad_q \n",
    "\n",
    "            # Full-step for position (drift)\n",
    "            q = q + self.a**2 * p * dt\n",
    "            #q = q + p * dt\n",
    "\n",
    "            # Compute new grad_y for the next iteration\n",
    "            V = self.potential_energy(q)\n",
    "            grad_q, = grad(V.sum(), q, create_graph=True)\n",
    "\n",
    "            # Final half-step for momentum (kick)\n",
    "            p = p - 0.5 * dt * grad_q \n",
    "\n",
    "        return q, p\n",
    "\n",
    "    def forward(self, q, p, cond):\n",
    "        sigma_q, sigma_p = cond[:,0].unsqueeze(1), cond[:,1].unsqueeze(1)\n",
    "\n",
    "        q.requires_grad, p.requires_grad = True, True  # Ensure gradients\n",
    "\n",
    "        # Perform Leapfrog steps\n",
    "        q, p = self.leapfrog_integrator(q, p, self.L, self.dt)\n",
    "        return q, p, sigma_q, sigma_p\n",
    "    \n",
    "    def loss(self, q, p, cond):\n",
    "        q0, p0, sigma_q, sigma_p = self.forward(q, p, cond)\n",
    "        \n",
    "        # Prior: Negative log of Gaussian base distribution\n",
    "        # Create a standard normal distribution\n",
    "        prior_q = D.Normal(loc=64.0, scale=sigma_q)\n",
    "        prior_p = D.Normal(loc=0.0, scale=sigma_p)\n",
    "        log_pi_q0 = (prior_q.log_prob(q0)).sum(dim=1)\n",
    "        log_pi_p0 = (prior_p.log_prob(p0)).sum(dim=1)  \n",
    "        \n",
    "        # KL Loss\n",
    "        return -(log_pi_q0 + log_pi_p0).mean()\n",
    "\n",
    "    def sample(self, q0, p0, nsteps, delta_t):\n",
    "        q0.requires_grad, p0.requires_grad = True, True\n",
    "        q0, p0 = q0.unsqueeze(0), p0.unsqueeze(0)\n",
    "\n",
    "        q, p = self.leapfrog_integrator(q0, p0, nsteps, -delta_t)\n",
    "        return q.detach(), p.detach()\n",
    "\n",
    "\n",
    "L = 25 # Number of Leapfrog steps\n",
    "dt = -0.04 # Integration timestep\n",
    "model = NeuralHamiltonianFlow(L=L, dt=dt)\n",
    "model.to(device)\n",
    "model.double()\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"The model is made of \" + str(n_parameters) + \" parameters.\")\n",
    "model.load_state_dict(torch.load(folder_exp+'model_final', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e99f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT LOSSES\n",
    "\n",
    "training_loss = np.load(folder_exp+'training_loss_final.npy')\n",
    "validation_loss = np.load(folder_exp+'validation_loss_final.npy')\n",
    "plt.plot(training_loss, label='training')\n",
    "plt.plot(validation_loss, label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE TEST EXAMPLE\n",
    "\n",
    "def wasserstein_distance_1d(a, b):\n",
    "    a_sorted, _ = torch.sort(a)\n",
    "    b_sorted, _ = torch.sort(b)\n",
    "    return torch.mean(torch.abs(a_sorted - b_sorted))\n",
    "\n",
    "k = np.random.randint(N_test)\n",
    "#k = 2709\n",
    "print(k)\n",
    "true_q_10, true_p_10 = Q010_test[k], P010_test[k]\n",
    "true_q_20, true_p_20 = Q020_test[k], P020_test[k]\n",
    "true_q_30, true_p_30 = Q030_test[k], P030_test[k]\n",
    "true_q_40, true_p_40 = Q040_test[k], P040_test[k]\n",
    "true_q_50, true_p_50 = Q050_test[k], P050_test[k]\n",
    "\n",
    "import time\n",
    "sample_q_10, sample_p_10 = model.sample(Q000_test[k], P000_test[k], L//5, dt)\n",
    "sample_q_20, sample_p_20 = model.sample(Q000_test[k], P000_test[k], 2*L//5, dt)\n",
    "sample_q_30, sample_p_30 = model.sample(Q000_test[k], P000_test[k], 3*L//5, dt)\n",
    "sample_q_40, sample_p_40 = model.sample(Q000_test[k], P000_test[k], 4*L//5, dt)\n",
    "t0 = time.time()\n",
    "sample_q_50, sample_p_50 = model.sample(Q000_test[k], P000_test[k], L, dt)\n",
    "t1 = time.time()\n",
    "print(t1-t0)\n",
    "\n",
    "H_00 = model.potential_energy(Q000_test[k].unsqueeze(0)) + (model.a**2*P000_test[k].unsqueeze(0)**2/2).sum()\n",
    "H_10 = model.potential_energy(sample_q_10) + (model.a**2*sample_p_10**2/2).sum()\n",
    "H_20 = model.potential_energy(sample_q_20) + (model.a**2*sample_p_20**2/2).sum()\n",
    "H_30 = model.potential_energy(sample_q_30) + (model.a**2*sample_p_30**2/2).sum()\n",
    "H_40 = model.potential_energy(sample_q_40) + (model.a**2*sample_p_40**2/2).sum()\n",
    "H_50 = model.potential_energy(sample_q_50) + (model.a**2*sample_p_50**2/2).sum()\n",
    "H_00 = H_00.cpu()\n",
    "H_10 = H_10.cpu()\n",
    "H_20 = H_20.cpu()\n",
    "H_30 = H_30.cpu()\n",
    "H_40 = H_40.cpu()\n",
    "H_50 = H_50.cpu()\n",
    "print(Q000_test[k].std())\n",
    "print(P000_test[k].std())\n",
    "print((H_50-H_00)/H_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d9ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT ENERGY EVOLUTION OF TEST EXAMPLE\n",
    "\n",
    "plt.figure()\n",
    "plt.plot([H_00.detach().numpy(), H_10.detach().numpy(), H_20.detach().numpy(), H_30.detach().numpy(), H_40.detach().numpy(), H_50.detach().numpy()])\n",
    "#plt.ylim([-2500,-1800])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CUMULATIVE HISTOGRAMS OF TEST EXAMPLE\n",
    "\n",
    "qmin, qmax, pmin, pmax, nbins = 55, 75, -12, 12, 20 \n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) = plt.subplots(5, 2, sharex='col', sharey='row', \n",
    "                                                                    gridspec_kw={'hspace': 0.1, 'wspace': 0.02},\n",
    "                                                                    constrained_layout=True)\n",
    "for ax in [ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8,ax9,ax10]:  # Loop over all axes\n",
    "    ax.tick_params(axis='both', labelsize=14)  # Set font size for both x and y ticks\n",
    "\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(8)\n",
    "# First row\n",
    "ax1.hist(true_q_10.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='blue', label='True')\n",
    "ax1.hist(sample_q_10.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='red', label='Sampled')\n",
    "ax1.legend(loc='upper left', fontsize=16)\n",
    "ax1.set_xlim([qmin,qmax])\n",
    "ax1.set_ylabel('t=0.2', fontsize=22)\n",
    "ax2.hist(true_p_10.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='blue', label='True p-density')\n",
    "ax2.hist(sample_p_10.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='red', label='Generated p-density')\n",
    "ax2.set_xlim([pmin,pmax])\n",
    "#ax2.legend(loc='upper left')\n",
    "# Second row\n",
    "ax3.hist(true_q_20.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='blue', label='True q-density')\n",
    "ax3.hist(sample_q_20.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='red', label='Generated q-density')\n",
    "ax3.set_xlim([qmin,qmax])\n",
    "ax3.set_ylabel('t=0.4', fontsize=22)\n",
    "ax4.hist(true_p_20.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='blue', label='True p-density')\n",
    "ax4.hist(sample_p_20.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='red', label='Generated p-density')\n",
    "ax4.set_xlim([pmin,pmax])\n",
    "# Third row\n",
    "ax5.hist(true_q_30.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='blue', label='True q-density')\n",
    "ax5.hist(sample_q_30.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='red', label='Generated q-density')\n",
    "ax5.set_xlim([qmin,qmax])\n",
    "ax5.set_ylabel('t=0.6', fontsize=22)\n",
    "ax6.hist(true_p_30.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='blue', label='True p-density')\n",
    "ax6.hist(sample_p_30.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='red', label='Generated p-density')\n",
    "ax6.set_xlim([pmin,pmax])\n",
    "# Fourth row\n",
    "ax7.hist(true_q_40.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='blue', label='True q-density')\n",
    "ax7.hist(sample_q_40.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='red', label='Generated q-density')\n",
    "ax7.set_xlim([qmin,qmax])\n",
    "ax7.set_ylabel('t=0.8', fontsize=22)\n",
    "ax8.hist(true_p_40.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='blue', label='True p-density')\n",
    "ax8.hist(sample_p_40.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='red', label='Generated p-density')\n",
    "ax8.set_xlim([pmin,pmax])\n",
    "# Fifth row\n",
    "ax9.hist(true_q_50.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='blue', label='True q-density')\n",
    "ax9.hist(sample_q_50.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='red', label='Generated q-density')\n",
    "ax9.set_xlim([qmin,qmax])\n",
    "ax9.set_ylabel('t=1.0', fontsize=22)\n",
    "ax9.set_xlabel('Positions', fontsize=22)\n",
    "ax10.hist(true_p_50.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='blue', label='True p-density')\n",
    "ax10.hist(sample_p_50.squeeze(0).detach().cpu(), \n",
    "         density=True, cumulative=True, histtype='step', bins=nbins, linewidth=2.0,\n",
    "         color='red', label='Generated p-density')\n",
    "ax10.set_xlim([pmin,pmax])\n",
    "ax10.set_xlabel('Momenta', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e639ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT STATISTICS ACROSS TRAJECTORY OF TEST EXAMPLE\n",
    "\n",
    "print(\"####### L = 10 #######\")\n",
    "print(true_q_10.mean())\n",
    "print(true_q_10.std())\n",
    "print(sample_q_10.mean())\n",
    "print(sample_q_10.std())\n",
    "print('###############')\n",
    "print(true_p_10.mean())\n",
    "print(true_p_10.std())\n",
    "print(sample_p_10.mean())\n",
    "print(sample_p_10.std())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"####### L = 20 #######\")\n",
    "print(true_q_20.mean())\n",
    "print(true_q_20.std())\n",
    "print(sample_q_20.mean())\n",
    "print(sample_q_20.std())\n",
    "print('###############')\n",
    "print(true_p_20.mean())\n",
    "print(true_p_20.std())\n",
    "print(sample_p_20.mean())\n",
    "print(sample_p_20.std())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"####### L = 30 #######\")\n",
    "print(true_q_30.mean())\n",
    "print(true_q_30.std())\n",
    "print(sample_q_30.mean())\n",
    "print(sample_q_30.std())\n",
    "print('###############')\n",
    "print(true_p_30.mean())\n",
    "print(true_p_30.std())\n",
    "print(sample_p_30.mean())\n",
    "print(sample_p_30.std())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"####### L = 40 #######\")\n",
    "print(true_q_40.mean())\n",
    "print(true_q_40.std())\n",
    "print(sample_q_40.mean())\n",
    "print(sample_q_40.std())\n",
    "print('###############')\n",
    "print(true_p_40.mean())\n",
    "print(true_p_40.std())\n",
    "print(sample_p_40.mean())\n",
    "print(sample_p_40.std())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"####### L = 50 #######\")\n",
    "print(true_q_50.mean())\n",
    "print(true_q_50.std())\n",
    "print(sample_q_50.mean())\n",
    "print(sample_q_50.std())\n",
    "print('###############')\n",
    "print(true_p_50.mean())\n",
    "print(true_p_50.std())\n",
    "print(sample_p_50.mean())\n",
    "print(sample_p_50.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc947835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE ERRORS ON FULL TEST DATASET\n",
    "\n",
    "error05_q, error05_p = 0., 0.\n",
    "error10_q, error10_p = 0., 0.\n",
    "error15_q, error15_p = 0., 0.\n",
    "error20_q, error20_p = 0., 0.\n",
    "error25_q, error25_p = 0., 0.\n",
    "for k in range(6384):\n",
    "    true_q_10, true_p_10 = Q010_test[k], P010_test[k]\n",
    "    true_q_20, true_p_20 = Q020_test[k], P020_test[k]\n",
    "    true_q_30, true_p_30 = Q030_test[k], P030_test[k]\n",
    "    true_q_40, true_p_40 = Q040_test[k], P040_test[k]\n",
    "    true_q_50, true_p_50 = Q050_test[k], P050_test[k]\n",
    "    sample_q_10, sample_p_10 = model.sample(Q000_test[k], P000_test[k], L//5, dt)\n",
    "    sample_q_20, sample_p_20 = model.sample(Q000_test[k], P000_test[k], 2*L//5, dt)\n",
    "    sample_q_30, sample_p_30 = model.sample(Q000_test[k], P000_test[k], 3*L//5, dt)\n",
    "    sample_q_40, sample_p_40 = model.sample(Q000_test[k], P000_test[k], 4*L//5, dt)\n",
    "    sample_q_50, sample_p_50 = model.sample(Q000_test[k], P000_test[k], L, dt)\n",
    "\n",
    "    error05_q += float(wasserstein_distance_1d(sample_q_10, Q010_test[k]))\n",
    "    error05_p += float(wasserstein_distance_1d(sample_p_10, P010_test[k]))\n",
    "\n",
    "    error10_q += float(wasserstein_distance_1d(sample_q_20, Q020_test[k]))\n",
    "    error10_p += float(wasserstein_distance_1d(sample_p_20, P020_test[k]))\n",
    "\n",
    "    error15_q += float(wasserstein_distance_1d(sample_q_30, Q030_test[k]))\n",
    "    error15_p += float(wasserstein_distance_1d(sample_p_30, P030_test[k]))\n",
    "\n",
    "    error20_q += float(wasserstein_distance_1d(sample_q_40, Q040_test[k]))\n",
    "    error20_p += float(wasserstein_distance_1d(sample_p_40, P040_test[k]))\n",
    "\n",
    "    error25_q += float(wasserstein_distance_1d(sample_q_50, Q050_test[k]))\n",
    "    error25_p += float(wasserstein_distance_1d(sample_p_50, P050_test[k]))\n",
    "    \n",
    "    if (k+1)%500 == 0: print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9523149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT THESE ERRRORS\n",
    "\n",
    "print(error05_q/6384)\n",
    "print(error05_p/6384)\n",
    "\n",
    "print(error10_q/6384)\n",
    "print(error10_p/6384)\n",
    "\n",
    "print(error15_q/6384)\n",
    "print(error15_p/6384)\n",
    "\n",
    "print(error20_q/6384)\n",
    "print(error20_p/6384)\n",
    "\n",
    "print(error25_q/6384)\n",
    "print(error25_p/6384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce3cc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
